{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree | Assignment\n",
        "\n",
        "Assignment Code: DA-AG-012\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qklgVP0_VhuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Answer:A Decision Tree is a supervised learning algorithm that works like a flowchart to classify data.Decision Tree divides data in nodes and keeps on splitting the data unit all points are of same class.\n",
        "\n",
        "How it Works\n",
        "- The Split: The tree starts at a \"Root Node\" (the top) and looks for the feature that best separates the data into distinct groups.\n",
        "- The Test: It uses mathematical metrics like Gini Impurity or Entropy to find the \"purest\" split—meaning it wants to group similar items together.\n",
        "-The Path: Data points travel down branches based on their characteristics (e.g., Is age > 30?).\n",
        "- The Result: The process repeats until it reaches a Leaf Node, which provides the final category or class."
      ],
      "metadata": {
        "id": "t9N-XsYLVpUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\n",
        "Answer: Entropy measures \"disorder\" or uncertainty; the tree calculates the entropy before and after a split to find the Information Gain, choosing the path that reduces chaos the most. Gini Impurity measures the probability of a random item being misclassified; it is the industry default because it is computationally faster, avoiding the complex logarithmic calculations required by Entropy. Both measures impact the tree by forcing it to test every possible feature and \"cut\" point to find the split that creates the most uniform, homogeneous groups.\n"
      ],
      "metadata": {
        "id": "WxoOdigZX8VG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "\n",
        "Answer: Pruning is the essential process of trimming a Decision Tree to prevent it from \"overfitting,\" which occurs when the model becomes too complex and memorizes noise instead of learning patterns. Pre-Pruning, stops the tree's growth during the training process by setting specific constraints, such as a maximum depth or a minimum number of samples per leaf.Its primary practical advantage is computational efficiency, as it saves time and memory by never building an unnecessarily large tree. Post-Pruning allows the tree to grow to its full, complex height first and then trims away non-significant branches from the bottom up based on their performanc.Its main practical advantage is higher accuracy, as it evaluates the \"big picture\" and avoids prematurely cutting off splits that might have seemed weak initially but led to significant discoveries deeper in the tree.\n"
      ],
      "metadata": {
        "id": "DqtnU5HRYruc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Answer: Information Gain is the measurement of how much \"uncertainty\" or \"chaos\" is reduced in a dataset after a specific split is made. It is calculated by taking the Entropy (disorder) of the parent node and subtracting the weighted average Entropy of the resulting child nodes. The ultimate goal of a classification tree is to reach \"leaf nodes\" where every item belongs to the same class. Information Gain is the mathematical tool that drives the data toward that state of 100% purity."
      ],
      "metadata": {
        "id": "TSlsCpv4Zjhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Answer: Decision Trees are popular because they mirror human \"If-Then\" logic, but they come with specific trade-offs.\n",
        "\n",
        "Real-World Applications:\n",
        "- Finance: Credit scoring and fraud detection.\n",
        "- Healthcare: Patient risk stratification and symptom-based diagnosis.\n",
        "- Retail: Customer segmentation and \"churn\" prediction (predicting who will quit a service).\n",
        "\n",
        "Advantages:\n",
        "- High Interpretability\n",
        "- Versatile Data Handling\n",
        "- No Scaling Required\n",
        "- Automatic Feature Selection\n",
        "\n",
        "Limitations:\n",
        "- Overfitting\n",
        "- Instability (High Variance)\n",
        "- Bias toward \"Tall\" Features\n",
        "- Greedy Nature"
      ],
      "metadata": {
        "id": "SKyVv6C9Z7Hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier using the Gini criterion\n",
        "- Print the model’s accuracy and feature importances\n"
      ],
      "metadata": {
        "id": "5dPmTO8PbUN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer of 6th Question\n",
        "\n",
        "#importing useful libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#loading dataset and creating DataFrame\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns=data.feature_names)\n",
        "\n",
        "# Dividing data to independent and target variables\n",
        "X = df\n",
        "y = data.target\n",
        "\n",
        "# splitting in train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
        "\n",
        "# model building\n",
        "classifier = DecisionTreeClassifier(criterion='gini')\n",
        "classifier.fit(x_train,y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# model's accuracy\n",
        "print(f'Accuracy = {accuracy_score(y_test,y_pred)}')\n",
        "\n",
        "# Feature importance\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(data.feature_names, classifier.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n",
        "\n",
        "\n",
        "# ● Boston Housing Dataset for regression tasks\n",
        "# (sklearn.datasets.load_boston() or provided CSV).\n"
      ],
      "metadata": {
        "id": "uQdPQMTqblIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab1ca20-e194-4f4e-ed72-bd3057ad71a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.9555555555555556\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0215\n",
            "sepal width (cm): 0.0215\n",
            "petal length (cm): 0.0632\n",
            "petal width (cm): 0.8939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "GwywSVvlb1HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer to 7th Question\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a shallow Decision Tree (max_depth=3)\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf.fit(X_train, Y_train)\n",
        "Y_pred = clf.predict(X_test)\n",
        "accuracy_clf = accuracy_score(Y_test, Y_pred)\n",
        "\n",
        "# Train a fully-grown Decision Tree (no max_depth limit)\n",
        "clf_full = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf_full.fit(X_train, Y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# comparison\n",
        "print(f\"Tree (max_depth=3) Accuracy: {accuracy_clf:.2f}\")\n",
        "print(f\"Fully-grown Tree Accuracy: {accuracy_full:.2f}\")"
      ],
      "metadata": {
        "id": "nxv9gMfMb4Js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3b9240-6cc6-4fe8-c8ea-0760d169af16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree (max_depth=3) Accuracy: 1.00\n",
            "Fully-grown Tree Accuracy: 0.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "- Load the Boston Housing Dataset\n",
        "- Train a Decision Tree Regressor\n",
        "- Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "fj0Ez_mOb7PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Answer to 8th Question\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset from OpenML\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eTHcutn3cB10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56eddd3a-6716-4b8a-8ae6-24c4a327f30b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 11.59\n",
            "Feature Importances:\n",
            "CRIM: 0.0585\n",
            "ZN: 0.0010\n",
            "INDUS: 0.0099\n",
            "CHAS: 0.0003\n",
            "NOX: 0.0071\n",
            "RM: 0.5758\n",
            "AGE: 0.0072\n",
            "DIS: 0.1096\n",
            "RAD: 0.0016\n",
            "TAX: 0.0022\n",
            "PTRATIO: 0.0250\n",
            "B: 0.0119\n",
            "LSTAT: 0.1900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "- Print the best parameters and the resulting model accuracy\n"
      ],
      "metadata": {
        "id": "Iy9HAO7EcGIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer to 9th Question\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define the Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "id": "7kzNEltucMaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1667c5a-1226-4f2f-fadf-1a5a3870f132"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Answer:\n",
        "- Handle Missing Values:\n",
        "Impute numerical features with mean/median,Fill categorical features with mode or “Unknown\",Add missing indicators if clinically relevant.\n",
        "- Encode Categorical Features: Use one-hot encoding for nominal variables,Use label encoding for ordinal variables.\n",
        "- Train Decision Tree Model: Split data into train/test sets,Fit a DecisionTreeClassifier on the training set.\n",
        "- Tune Hyperparameters: Use GridSearchCV or RandomizedSearchCV to optimize max_depth, min_samples_split, and min_samples_leaf.\n",
        "- Evaluate Performance:  Check accuracy, precision, recall, F1-score, and ROC-AUC,Prioritize recall (sensitivity) in healthcare to minimize missed disease cases.\n",
        "- Business Value: Early detection of disease risk,Better resource allocation for hospitals,Improved patient outcomes through timely intervention,Cost savings by reducing unnecessary tests,Trustworthy decision support thanks to interpretable tree rules.\n"
      ],
      "metadata": {
        "id": "VYgi6uXPcRcj"
      }
    }
  ]
}