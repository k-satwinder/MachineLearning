{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Feature\n",
        "Engineering***\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KyZpdLeeOvri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: A parameter in machine learning is a variable inside a model that gets learned from data during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "KhUVdCu-Ox9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "What does negative correlation mean?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: Correlation is a statistical measure that describes the relationship between two variables. Correlation coefficient ranges from -1 to 1\n",
        "- r = +1, Perfect positive correlation\n",
        "- r = -1, Perfect negative correlation\n",
        "- r = 0, No correlation.\n",
        "Negative correlation means that as one variable increases, the other decreases.\n",
        "Correlation is vital for feature selection, identifying redundant features and understanding how variable are connected to improve model accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "CYCJLUGJOyOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: A computer program is said to learn from experience E with respect to some tasks T and perfromance measure P if it's perfromance at task T measured by P improves with experience E.Example, Task T - classifying email spam/ham , Experience E - label of mail, Performance measure P - no. of emails correctly classified.\n",
        "Core Components Of Machine Learning are:\n",
        "- Data - includes quality and quantity\n",
        "- Algorithmns - mathematical procedures for creating model\n",
        "- Models - representation of data patterns\n",
        "- Trainig data - process of converting input for meaningful predictive data\n",
        "- Test data - for testing perfromance of data."
      ],
      "metadata": {
        "id": "GJj3PPkbOycN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: Loss value measures how far predictions are from actual outcomes. A low loss indicates the model is performing well, while a high loss suggests poor fit. Comparing training and validation loss also reveals issues like underfitting or overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "TX_P4qnrOypn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: Continuous variables are numeric values that can take any value within a range, like height or weight. Categorical variables represent distinct groups or labels, like gender or product type. Continuous data is measured, while categorical data is classified.\n",
        "\n"
      ],
      "metadata": {
        "id": "fz5eTzVJOy16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "\n",
        "---\n",
        "Ans:\n",
        "1. One-Hot Encoding (OHE)-Creates new binary columns for each unique category, marking presence with 1 and absence with 0.Nominal Data (Categories with no inherent order), e.g., Color = {Red, Blue, Green} - creates Color_Red, Color_Blue, Color_Green columns.\n",
        "2. Label EncodingAssigns a unique integer to each category.Ordinal Data (Categories with a natural order), e.g., T-shirt Size = {Small, Medium, Large}-  encoded as - 0, 1, 2$.\n",
        "3. Target EncodingReplaces a category with the mean of the target variable for that category.High Cardinality features (many unique categories, like Zip Codes), as it doesn't increase dimensionality."
      ],
      "metadata": {
        "id": "GqVt9CHuOzC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "\n",
        "---\n",
        "Ans:\n",
        "- Trainig data is the portion of dataset that is used to teach the model\n",
        "- Testing data is the portion of dataset on which evaluation of training model is done to check model performance. If 1000 rows of data is their then 70% will used for training and left data will used for testing purpose.\n"
      ],
      "metadata": {
        "id": "DrOTaZQ8OzOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: The sklearn.preprocessing module in scikit-learn library is a collection of tools designed to tranform and prepare raw feature vectors into a representation that is more suitable for machine learning models."
      ],
      "metadata": {
        "id": "EZoPnjAjOzYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "\n",
        "\n",
        "---\n",
        "Ans:A test set is a portion of a dataset that is held back and used only to provide a final evaluation of a trained machine learning model.\n"
      ],
      "metadata": {
        "id": "F5ADx6ceOzja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: The most common and standard way to split data into training and testing sets in Python is by using the train_test_split function from the scikit-learn (or sklearn) library.\n",
        "The way to approach machine learning problem:\n",
        " - Defining the Problem\n",
        " - ata Acquisition and Exploration (EDA)\n",
        " - Data Preprocessing and Feature Engineering\n",
        " - Splitting the Data.\n",
        " - Model Training and Selection\n",
        " - Model Evaluation and Prediction\n",
        " - Deployment and Monitoring"
      ],
      "metadata": {
        "id": "Pj4WST-xOzts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: Because it provides the necessary understanding and cleanup required to ensure the model learns correctly, efficiently, and produces reliable results.\n"
      ],
      "metadata": {
        "id": "FCy89n7VOz37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: Correlation is a statistical measure that describes the relationship between two variables. Correlation coefficient ranges from -1 to 1\n",
        "- r = +1, Perfect positive correlation\n",
        "- r = -1, Perfect negative correlation\n",
        "- r = 0, No correlation.\n",
        "Correlation is vital for feature selection, identifying redundant features and understanding how variable are connected to improve model accuracy.\n"
      ],
      "metadata": {
        "id": "z8gAZJ4UO0C2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: Negative correlation indicates if one variable increases the other decreases propotionally.\n"
      ],
      "metadata": {
        "id": "c9KcslpFO0NW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "\n",
        "---\n",
        "Ans:\n",
        "Can find the correlation between variables in Python primarily using the .corr() method in the Pandas library.\n",
        "```\n",
        "import pandas as pd\n",
        "data = {'A': [1, 2, 3, 4],\n",
        "        'B': [5, 6, 7, 8],\n",
        "        'C': [8, 6, 4, 2]}\n",
        "df = pd.DataFrame(data)\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HDcjOz1HO0XA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "\n",
        "---\n",
        "Ans: Causation is a relationship where a change in one variable directly produces a change in another variable\n",
        "- Definition: Correlation -\tVariables are statistically associated; they move together.\tCausation - A change in Variable A forces a change in Variable B.\n",
        "- Implication:\tCorrelation does not imply causation.\tCausation always implies correlation.\n",
        "-Example:A study finds a strong correlation between Ice Cream Sales and the number of Drowning Incidents.Correlation: Both variables increase and decrease together.Causation: Ice cream does not cause drowning."
      ],
      "metadata": {
        "id": "-MvISME4O0hW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "\n",
        "---\n",
        "Ans: An Optimizer is an algorithm or method used in machine learning and deep learning to adjust the parameters (weights and biases) of a model iteratively during training. Its main goal is to minimize the loss function (or error function), guiding the model toward the optimal set of parameters that yield the most accurate results.\n",
        "- Gradient Descent (GD) Variants:\n",
        "These optimizers calculate the gradient of the loss function to determine the direction of the steepest descent.Example-Training a neural network on a massive dataset where you update the weights after processing every 50 data points\n",
        "- Adaptive Learning Rate Optimizers:\n",
        "These optimizers dynamically adjust the learning rate for each parameter individually during training. Example: In a text classification task where one word appears very often and another is rare, AdaGrad assigns a smaller learning rate to the frequent word's corresponding weight to prevent it from overshooting the minimum, and a larger one to the rare word's weight to ensure it learns enough."
      ],
      "metadata": {
        "id": "q9VNqsNXO0ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "\n",
        "\n",
        "---\n",
        "Ans:\n",
        "The sklearn.linear_model module is a vital component of the Scikit-learn (sklearn) machine learning library in Python.\n",
        "It contains a collection of classes and functions dedicated to implementing various linear models for both regression and classification tasks."
      ],
      "metadata": {
        "id": "I4jk6aAMO006"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: The model.fit() method trains a machine learning model by iteratively adjusting its internal parameters (weights and biases) to minimize the loss on the provided training data.\n",
        "The following two arguments must given:\n",
        "- Training Input Data (X or x): The input features or samples the model learns from.\n",
        "- Target Output Data (y): The true labels or target values corresponding to the input data.\n"
      ],
      "metadata": {
        "id": "4HlAV_VdO0_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: The model.predict() method generates output predictions using a trained model on new, unseen input data.It requires only one primary argument:\n",
        "\n",
        "- New Input Data (X or x): The features/samples for which you want the model to generate an output (e.g., class probabilities for classification or numerical values for regression).\n"
      ],
      "metadata": {
        "id": "dihyDg1bO1IL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: Continuous variables are numeric values that can take any value within a range, like height or weight. Categorical variables represent distinct groups or labels, like gender or product type. Continuous data is measured, while categorical data is classified.\n"
      ],
      "metadata": {
        "id": "0_jEKP2aO1Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Ans: Feature scaling is a data preprocessing technique used to standardize or normalize the range of independent features (variables) in a dataset.\n",
        "Feature scaling is a method of changing the data values so that they fit within a specified scale, such as 0 or -1 to 1 , or transforming the data so that it has properties like a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "9Y8UJCWsO1cJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: The standard, correct workflow involves using the .fit() method on your training data to calculate the necessary statistics (like mean, standard deviation, min, and max) and then using the .transform() method to apply the scaling to both your training and test data.\n",
        "Example:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KxJszUoyO1ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: The sklearn.preprocessing module in scikit-learn library is a collection of tools designed to tranform and prepare raw feature vectors into a representation that is more suitable for machine learning models."
      ],
      "metadata": {
        "id": "6bhQivODO1xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: To split data for model fitting in Python using the train_test_split function from the Scikit-learn library.\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "test_size_ratio = 0.2\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=test_size_ratio,\n",
        "    random_state=42 # Ensures the same split every time\n",
        ")\n",
        "print(f\"Training set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3V0qTMfAO19J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "\n",
        "\n",
        "---\n",
        "Ans: Data encoding is simply translating text-based categories (like 'Red', 'London', or 'High') into numbers so that machine learning models can process them.The Two Main Methods:\n",
        "- Label Encoding:nAssigns a single integer to each category (e.g., Small to 1, Medium to 2).Use for: Data that has a meaningful order (Ordinal data).\n",
        "- One-Hot Encoding: Creates a new column for each unique category, marking the presence with 1 and absence with 0.Use for: Data with no order (Nominal data, like colors). This prevents the model from assuming an incorrect numerical relationship between categories.\n"
      ],
      "metadata": {
        "id": "mCtouOXKrGY8"
      }
    }
  ]
}